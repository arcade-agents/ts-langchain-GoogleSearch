"use strict";
import { getTools, confirm, arcade } from "./tools";
import { createAgent } from "langchain";
import {
  Command,
  MemorySaver,
  type Interrupt,
} from "@langchain/langgraph";
import chalk from "chalk";
import * as readline from "node:readline/promises";

// configure your own values to customize your agent

// The Arcade User ID identifies who is authorizing each service.
const arcadeUserID = process.env.ARCADE_USER_ID;
if (!arcadeUserID) {
  throw new Error("Missing ARCADE_USER_ID. Add it to your .env file.");
}
// This determines which MCP server is providing the tools, you can customize this to make a Slack agent, or Notion agent, etc.
// all tools from each of these MCP servers will be retrieved from arcade
const toolkits=['GoogleSearch'];
// This determines isolated tools that will be
const isolatedTools=[];
// This determines the maximum number of tool definitions Arcade will return
const toolLimit = 100;
// This prompt defines the behavior of the agent.
const systemPrompt = `# Introduction

Welcome to the AI Research Assistant! This agent is designed to help you conduct efficient online searches and gather valuable information from Google. By leveraging advanced search capabilities, it provides timely answers to your queries, making your research process smoother and more effective.

# Instructions

1. **Understand the Query:** Interpret the user's search query to identify key terms and context.
2. **Conduct a Google Search:** Utilize the GoogleSearch_Search tool by sending a query to retrieve relevant organic search results.
3. **Analyze Results:** Review the search results to extract key information and present it to the user in a clear and concise manner.
4. **Follow Up:** If additional information or clarification is needed, engage further with follow-up questions or searches.
5. **Provide Summaries:** Offer summaries or synthesized content based on the search results to help the user understand the information quickly.

# Workflows

## Workflow 1: Basic Information Search
1. Receive user query.
2. Use **GoogleSearch_Search** to perform a search with the specified query.
3. Analyze the top search results and extract key information.
4. Present the findings to the user.

## Workflow 2: Topic Exploration
1. Receive user query for a broader topic.
2. Use **GoogleSearch_Search** to retrieve multiple results (set `n_results` to a higher number).
3. Categorize the results based on subtopics or relevant themes.
4. Summarize the subtopics and provide a comprehensive overview to the user.

## Workflow 3: Follow-Up Clarification
1. Identify if user seeks further information after initial search results.
2. Engage user with clarifying questions to refine their query.
3. Conduct a follow-up search via **GoogleSearch_Search** with the refined query.
4. Analyze and present new findings to the user.

## Workflow 4: Comparative Analysis
1. Receive user query pertaining to comparisons between different entities (e.g., products, services).
2. Use **GoogleSearch_Search** to gather relevant results for all entities involved in the comparison.
3. Extract key comparative points from the search results.
4. Present a comparative synopsis to the user highlighting the differences and similarities.`;
// This determines which LLM will be used inside the agent
const agentModel = process.env.OPENAI_MODEL;
if (!agentModel) {
  throw new Error("Missing OPENAI_MODEL. Add it to your .env file.");
}
// This allows LangChain to retain the context of the session
const threadID = "1";

const tools = await getTools({
  arcade,
  toolkits: toolkits,
  tools: isolatedTools,
  userId: arcadeUserID,
  limit: toolLimit,
});



async function handleInterrupt(
  interrupt: Interrupt,
  rl: readline.Interface
): Promise<{ authorized: boolean }> {
  const value = interrupt.value;
  const authorization_required = value.authorization_required;
  const hitl_required = value.hitl_required;
  if (authorization_required) {
    const tool_name = value.tool_name;
    const authorization_response = value.authorization_response;
    console.log("‚öôÔ∏è: Authorization required for tool call", tool_name);
    console.log(
      "‚öôÔ∏è: Please authorize in your browser",
      authorization_response.url
    );
    console.log("‚öôÔ∏è: Waiting for you to complete authorization...");
    try {
      await arcade.auth.waitForCompletion(authorization_response.id);
      console.log("‚öôÔ∏è: Authorization granted. Resuming execution...");
      return { authorized: true };
    } catch (error) {
      console.error("‚öôÔ∏è: Error waiting for authorization to complete:", error);
      return { authorized: false };
    }
  } else if (hitl_required) {
    console.log("‚öôÔ∏è: Human in the loop required for tool call", value.tool_name);
    console.log("‚öôÔ∏è: Please approve the tool call", value.input);
    const approved = await confirm("Do you approve this tool call?", rl);
    return { authorized: approved };
  }
  return { authorized: false };
}

const agent = createAgent({
  systemPrompt: systemPrompt,
  model: agentModel,
  tools: tools,
  checkpointer: new MemorySaver(),
});

async function streamAgent(
  agent: any,
  input: any,
  config: any
): Promise<Interrupt[]> {
  const stream = await agent.stream(input, {
    ...config,
    streamMode: "updates",
  });
  const interrupts: Interrupt[] = [];

  for await (const chunk of stream) {
    if (chunk.__interrupt__) {
      interrupts.push(...(chunk.__interrupt__ as Interrupt[]));
      continue;
    }
    for (const update of Object.values(chunk)) {
      for (const msg of (update as any)?.messages ?? []) {
        console.log("ü§ñ: ", msg.toFormattedString());
      }
    }
  }

  return interrupts;
}

async function main() {
  const config = { configurable: { thread_id: threadID } };
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  console.log(chalk.green("Welcome to the chatbot! Type 'exit' to quit."));
  while (true) {
    const input = await rl.question("> ");
    if (input.toLowerCase() === "exit") {
      break;
    }
    rl.pause();

    try {
      let agentInput: any = {
        messages: [{ role: "user", content: input }],
      };

      // Loop until no more interrupts
      while (true) {
        const interrupts = await streamAgent(agent, agentInput, config);

        if (interrupts.length === 0) {
          break; // No more interrupts, we're done
        }

        // Handle all interrupts
        const decisions: any[] = [];
        for (const interrupt of interrupts) {
          decisions.push(await handleInterrupt(interrupt, rl));
        }

        // Resume with decisions, then loop to check for more interrupts
        // Pass single decision directly, or array for multiple interrupts
        agentInput = new Command({ resume: decisions.length === 1 ? decisions[0] : decisions });
      }
    } catch (error) {
      console.error(error);
    }

    rl.resume();
  }
  console.log(chalk.red("üëã Bye..."));
  process.exit(0);
}

// Run the main function
main().catch((err) => console.error(err));